{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db8e719",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb294ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os, sys\n",
    "import math\n",
    "import seaborn as sns\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as Func\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.dirname(os.path.realpath('.')))\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a829a8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.supernet import SuperNet \n",
    "from helpers.gradcam import NetworkGradCAM\n",
    "from helpers.xrai import XRai\n",
    "#from helpers.classynet import ClassyNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649631a",
   "metadata": {},
   "source": [
    "# Data Read-Ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "841f4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = '/home/jmryan/private/DSC180/A/test/testdata.csv'\n",
    "TRAIN_PATH = '/home/jmryan/private/DSC180/A/train/traindata.csv'\n",
    "VAL_PATH = '/home/jmryan/private/DSC180/A/val/valdata.csv'\n",
    "\n",
    "FULL_HSIAO_PATH = '/home/jmryan/teams/dsc-180a---a14-[88137]/segmented_datapaths_meta.csv'\n",
    "HSIAO_DIR_PATH = '/home/jmryan/teams/dsc-180a---a14-[88137]/bnpp_224_pandas/'\n",
    "HSIAO_LUNG_PATH = '/home/jmryan/teams/dsc-180a---a14-[88137]/seg_lung_224_pandas/'\n",
    "HSIAO_HEART_PATH = '/home/jmryan/teams/dsc-180a---a14-[88137]/seg_heart_224_pandas/'\n",
    "\n",
    "FULL_MIMIC_PATH = '/home/jmryan/teams/dsc-180a---a14-[88137]/final_mimic_paths.csv'\n",
    "MIMIC_DIR_PATH = '/home/jmryan/teams/dsc-180a---a14-[88137]/mimic_224_pandas/'\n",
    "MIMIC_LUNG_PATH = '/home/jmryan/teams/dsc-180a---a14-[88137]/mimic_seg_lung_224_pandas/'\n",
    "MIMIC_HEART_PATH = '/home/jmryan/teams/dsc-180a---a14-[88137]/mimic_seg_heart_224_pandas/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95bde9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(TEST_PATH, header=0, index_col=0)\n",
    "train = pd.read_csv(TRAIN_PATH, index_col = 0)\n",
    "val = pd.read_csv(VAL_PATH, index_col = 0)\n",
    "seg = pd.read_csv(FULL_HSIAO_PATH, index_col = 0)\n",
    "mim = pd.read_csv(FULL_MIMIC_PATH, index_col=0)\n",
    "seg['key'] = seg.filepaths.apply(lambda x: x.split('/')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e348110",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df, mimic, transform=None, target_transform=None, seg = False):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.seg = seg\n",
    "        if mimic:\n",
    "            self.path = MIMIC_DIR_PATH\n",
    "            if self.seg:\n",
    "                self.heart = MIMIC_HEART_PATH\n",
    "                self.lung = MIMIC_LUNG_PATH\n",
    "        else:\n",
    "            self.path = HSIAO_DIR_PATH\n",
    "            if self.seg:\n",
    "                self.heart = HSIAO_HEART_PATH\n",
    "                self.lung = HSIAO_LUNG_PATH\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df[idx]\n",
    "        filepath = row[1]\n",
    "        val = row[0]\n",
    "        if self.seg:\n",
    "            full = torch.load(self.path + filepath + '/' + filepath + '_224.pandas')\n",
    "            lung = torch.load(self.lung + filepath + '/' + filepath + '_224.pandas')\n",
    "            heart = torch.load(self.heart + filepath + '/' + filepath + '_224.pandas')\n",
    "            im = torch.stack([full, lung, heart])\n",
    "        else:\n",
    "            im = torch.load(self.path + filepath + '/' + filepath + '_224.pandas').view(1, 224, 224).expand(3, -1, -1)\n",
    "        return im, int(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0d56ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsiao_train, hsiao_val = train_test_split(seg[['heart', 'id']].to_numpy(), test_size=0.2, random_state=42)\n",
    "mim_train, temp = train_test_split(mim.to_numpy(), test_size=0.2, random_state=42)\n",
    "mim_val, mim_test = train_test_split(temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ce451ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsiao_full_train_dataset = ImageDataset(hsiao_train, mimic=False, seg=False)\n",
    "hsiao_full_val_dataset = ImageDataset(hsiao_val, mimic=False, seg=False)\n",
    "\n",
    "mim_full_train_dataset = ImageDataset(mim_train, mimic=True, seg=False)\n",
    "mim_full_val_dataset = ImageDataset(mim_val, mimic=True, seg=False)\n",
    "mim_full_test_dataset = ImageDataset(mim_test, mimic=True, seg=False)\n",
    "\n",
    "\n",
    "hsiao_seg_train_dataset = ImageDataset(hsiao_train, mimic=False, seg=True)\n",
    "hsiao_seg_val_dataset = ImageDataset(hsiao_val, mimic=False, seg=True)\n",
    "\n",
    "mim_seg_train_dataset = ImageDataset(mim_train, mimic=True, seg=True)\n",
    "mim_seg_val_dataset = ImageDataset(mim_val, mimic=True, seg=True)\n",
    "mim_seg_test_dataset = ImageDataset(mim_test, mimic=True, seg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b640cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hsiao_full_train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5749b316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mim_full_train_dataset[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "921f4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "hsiao_full_train_dl = DataLoader(hsiao_full_train_dataset, batch_size=BATCH_SIZE, num_workers=16, shuffle=True)\n",
    "hsiao_full_val_dl = DataLoader(hsiao_full_val_dataset, batch_size=BATCH_SIZE, num_workers = 16, shuffle=False)\n",
    "\n",
    "hsiao_seg_train_dl = DataLoader(hsiao_seg_train_dataset, batch_size=BATCH_SIZE, num_workers=16, shuffle=True)\n",
    "hsiao_seg_val_dl = DataLoader(hsiao_seg_val_dataset, batch_size=BATCH_SIZE, num_workers = 16, shuffle=False)\n",
    "\n",
    "mim_full_train_dl = DataLoader(mim_full_train_dataset, batch_size=BATCH_SIZE, num_workers=16, shuffle=True)\n",
    "mim_full_val_dl = DataLoader(mim_full_val_dataset, batch_size=BATCH_SIZE, num_workers = 16, shuffle=False)\n",
    "\n",
    "mim_seg_train_dl = DataLoader(mim_seg_train_dataset, batch_size=BATCH_SIZE, num_workers=16, shuffle=True)\n",
    "mim_seg_val_dl = DataLoader(mim_seg_val_dataset, batch_size=BATCH_SIZE, num_workers = 16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e1cf49",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31821d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import functional as Func\n",
    "from torchvision import transforms as T\n",
    "from torch.nn import functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms, utils\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, precision_score, accuracy_score\n",
    "from helpers.lightning_interface import *\n",
    "import matplotlib as mpl\n",
    "from skimage.transform import resize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe4545bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassyNet(SuperFace):\n",
    "    def __init__(self, title, layer_defs, linear_layers, type_model = 'classifier', lr = 1e-3, is_transfer=False, model=None, lr_scheduler = [], batch_size =32, print_on = True):\n",
    "        super().__init__(layer_defs = layer_defs, model = model, lr_scheduler = lr_scheduler, lr=lr)\n",
    "        \n",
    "        self.title = title\n",
    "        self.model = model\n",
    "        self.print = print_on\n",
    "        self.linear_layers = linear_layers\n",
    "        self.grad = False\n",
    "        self.val_heart_true_epoch = np.array([])\n",
    "        self.val_heart_hat_epoch = np.array([])\n",
    "        self.train_heart_true_epoch = np.array([])\n",
    "        self.train_heart_hat_epoch = np.array([])\n",
    "        self.val_mae_epoch = np.array([])\n",
    "        self.train_mae_epoch = np.array([])\n",
    "        self.train_loss_epoch = np.array([])\n",
    "        self.val_loss_epoch = np.array([])\n",
    "        self.val_auc = np.array([])\n",
    "        self.init_model()\n",
    "        \n",
    "#         self.l1_loss = nn.L1Loss()\n",
    "#         self.l2_loss = nn.MSELoss()\n",
    "#         self.l1_str = l1\n",
    "#         self.l2_str = l2\n",
    "    \n",
    "        \n",
    "        self.tr_fpath = self.get_savefname(dset='train')\n",
    "        self.val_fpath = self.get_savefname(dset='valid')\n",
    "        \n",
    "        if os.path.isfile(self.tr_fpath):\n",
    "            os.remove(self.tr_fpath)\n",
    "            os.remove(self.val_fpath)\n",
    "#             print('Experiment Title Already Exists: Choose New Title')\n",
    "#             assert False\n",
    "        \n",
    "        self.BATCH_SIZE = batch_size\n",
    "        if type_model == 'regressor':\n",
    "            self.loss_func = self.mae\n",
    "            \n",
    "        elif type_model == 'classifier':\n",
    "            self.loss_func = self.cbe\n",
    "            \n",
    "        else:\n",
    "            print('Invalid Loss Func: Not Implemented')\n",
    "            assert False\n",
    "            \n",
    "    def get_savefname(self, dset='train'):\n",
    "        return f\"epoch_logs/{self.title}_logs_{dset}.csv\"\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=0.9) \n",
    "        lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience = 3, verbose=True)\n",
    "        return {\"optimizer\": optim, \"lr_scheduler\": {'scheduler': lr_sched, 'monitor': 'val_auc', \n",
    "                                                    'interval': 'epoch'}}\n",
    "    \n",
    "    def init_model(self):\n",
    "        layers = list(self.model.children())\n",
    "        lin = layers[-1]\n",
    "        layers = layers[:-1]\n",
    "        temp_lin = [nn.Linear(num_in,num_out) for num_in, num_out in self.linear_layers]\n",
    "        total_lin = []\n",
    "        for lin in temp_lin[:-1]:\n",
    "            #total_lin.append(nn.Dropout(0.75))\n",
    "            total_lin.append(lin)\n",
    "            total_lin.append(nn.ReLU())\n",
    "            total_lin.append(nn.Dropout(0.75))\n",
    "        total_lin.append(temp_lin[-1])\n",
    "#         total_lin.append(nn.LogSoftmax())\n",
    "        self.regresser = nn.Sequential(*total_lin)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.grad:\n",
    "            represents = self.layers(x).flatten(1)\n",
    "            y_hat = self.regresser(represents)\n",
    "        else:\n",
    "            represents = x\n",
    "            for i in range(len(self.layers)):\n",
    "                if True:#self.current_epoch >= 5 and (i == 8 or i==7 or i ==6 or i==5 or i==4):\n",
    "                    represents = self.layers[i](represents)\n",
    "                    \n",
    "#                 elif self.current_epoch >= 10 and i == 7:\n",
    "#                     represents = self.layers[i](represents)\n",
    "                    \n",
    "#                 elif self.current_epoch >= 15 and i == 6:\n",
    "#                     represents = self.layers[i](represents)\n",
    "                    \n",
    "#                 elif self.current_epoch >= 20 and i == 5:\n",
    "#                     represents = self.layers[i](represents)\n",
    "\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        represents = self.layers[i](represents)\n",
    "            y_hat = self.regresser(represents.flatten(1))\n",
    "        del x\n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "    def turn_grad(self, boo):\n",
    "        self.grad = boo\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        y_hat = self(x)\n",
    "        #print(y_hat.detach().mean().item(), y.mean().item(), y_hat.detach().median().item(), y.median().item())\n",
    "        del x\n",
    "        \n",
    "#         loss = self.loss_func(y_hat, y)\n",
    "        \n",
    "        loss = self.loss_func(y_hat, y)\n",
    "        \n",
    "        arged = F.softmax(y_hat.detach())[:,1]\n",
    "        \n",
    "\n",
    "        \n",
    "        loss_dic = {'loss': loss,\n",
    "                    'y_hat': arged,\n",
    "                    'y_true': y\n",
    "                   }\n",
    "        \n",
    "        return loss_dic\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        y_hat = self(x)\n",
    "        #print(y_hat.detach().mean().item(), y.mean().item(), y_hat.detach().median().item(), y.median().item())\n",
    "        del x\n",
    "        \n",
    "#         loss = self.loss_func(y_hat, y)\n",
    "        \n",
    "        loss = self.loss_func(y_hat, y).detach()\n",
    "        \n",
    "        arged = F.softmax(y_hat.detach())[:,1]\n",
    "\n",
    "#         arged = self.softmax_np(y_hat.cpu().detach().numpy())[:,1]\n",
    "    \n",
    "#         print(f\"Arged: {arged}\")\n",
    "\n",
    "        \n",
    "        loss_dic = {'loss': loss,\n",
    "                    'y_hat': arged,\n",
    "                    'y_true': y\n",
    "                   }\n",
    "        \n",
    "        return loss_dic\n",
    "    \n",
    "        \n",
    "    def training_step_end(self, batch_loss):\n",
    "        total_loss = batch_loss['loss'].mean()\n",
    "        heart_true = batch_loss['y_true'].to('cpu').numpy()\n",
    "        heart_hat = batch_loss['y_hat'].to('cpu').numpy()\n",
    "        \n",
    "        self.train_heart_true_epoch = np.append(self.train_heart_true_epoch, heart_true)\n",
    "        self.train_heart_hat_epoch = np.append(self.train_heart_hat_epoch, heart_hat)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        out = 'training_step (pre del) mem %:', torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()\n",
    "        #print(out)        \n",
    "        return total_loss\n",
    "    \n",
    "    def validation_step_end(self, batch_loss):\n",
    "        total_loss = batch_loss['loss'].mean()\n",
    "        heart_true = batch_loss['y_true'].to('cpu').numpy()\n",
    "        heart_hat = batch_loss['y_hat'].to('cpu').numpy()\n",
    "        \n",
    "        \n",
    "        self.val_heart_true_epoch = np.append(self.val_heart_true_epoch, heart_true)\n",
    "        self.val_heart_hat_epoch = np.append(self.val_heart_hat_epoch, heart_hat)\n",
    "        \n",
    "        return total_loss\n",
    "        \n",
    "    def training_epoch_end(self, step_outputs):\n",
    "        \n",
    "        losses = [loss['loss'] for loss in step_outputs]\n",
    "        \n",
    "        heart_true = self.train_heart_true_epoch\n",
    "        heart_hat = self.train_heart_hat_epoch\n",
    "        total_loss = sum(losses)/len(losses)\n",
    "        #print(heart_hat)\n",
    "        auc = roc_auc_score(heart_true, heart_hat)\n",
    "        prc = precision_score(heart_true, (heart_hat > 0.5).astype(int), zero_division=0)\n",
    "        acc = accuracy_score(heart_true, (heart_hat > 0.5).astype(int))\n",
    "        \n",
    "# #         self.log('train_AUC', auc,\n",
    "#                 on_step=False, on_epoch=True, prog_bar=True, batch_size=self.BATCH_SIZE)\n",
    "        \n",
    "#         self.log('train_PRC', prc,\n",
    "#                 on_step=False, on_epoch=True, prog_bar=True, batch_size=self.BATCH_SIZE)\n",
    "        \n",
    "        self.log('loss', total_loss,\n",
    "                on_step=False, on_epoch=True, prog_bar=False, batch_size=self.BATCH_SIZE)\n",
    "        \n",
    "        \n",
    "        info_dic = {'Epoch': [self.current_epoch], 'AUC': [auc], 'PRC': [prc], 'Accuracy': [acc], 'loss':[total_loss.item()]}\n",
    "        print(f\"Epoch {self.current_epoch}\")\n",
    "        if self.print:\n",
    "            print(f\"\\tTrain {info_dic};\")\n",
    "#        print(f\"\\tTrain loss: {total_loss.item()}; mean_mae: {epoch_mae.mean()};\" + \n",
    " #            f\" mean_heart_hat: {heart_hat.mean()}; mean_heart_true: {heart_true.mean()}\")\n",
    "    \n",
    "        if not os.path.isfile(self.tr_fpath):\n",
    "            pd.DataFrame.from_dict(info_dic).to_csv(self.tr_fpath)\n",
    "            \n",
    "        else:\n",
    "            pd.concat([pd.read_csv(self.tr_fpath, index_col=0), pd.DataFrame.from_dict(info_dic)]).to_csv(self.tr_fpath)\n",
    "            \n",
    "        \n",
    "        self.train_loss_epoch = np.append(self.train_loss_epoch, total_loss.item())\n",
    "        \n",
    "        self.train_heart_true_epoch = np.array([])\n",
    "        self.train_heart_hat_epoch = np.array([])\n",
    "        #sch = self.lr_schedulers()\n",
    "        #print(self.lr_schedulers())\n",
    "\n",
    "        # If the selected scheduler is a ReduceLROnPlateau scheduler.\n",
    "\n",
    "        return None\n",
    "        \n",
    "    def validation_epoch_end(self, step_outputs):\n",
    "        losses = step_outputs\n",
    "\n",
    "        heart_true = self.val_heart_true_epoch\n",
    "        heart_hat = self.val_heart_hat_epoch\n",
    "        total_loss = sum(losses)/len(losses)\n",
    "        \n",
    "#         print(f'HAT: {heart_hat}')\n",
    "\n",
    "        auc = roc_auc_score(heart_true, heart_hat)\n",
    "        prc = precision_score(heart_true, (heart_hat > 0.5).astype(int), zero_division=0)\n",
    "        acc = accuracy_score(heart_true, (heart_hat > 0.5).astype(int))\n",
    "        self.log('val_auc', auc)\n",
    "\n",
    "#         self.log('val_AUC', auc,\n",
    "#                 on_step=False, on_epoch=True, prog_bar=True, batch_size=self.BATCH_SIZE)\n",
    "        \n",
    "#         self.log('val_PRC', prc,\n",
    "#                 on_step=False, on_epoch=True, prog_bar=True, batch_size=self.BATCH_SIZE)\n",
    "        \n",
    "# #         self.log('val_loss', total_loss,\n",
    "#                 on_step=False, on_epoch=True, prog_bar=True, batch_size=self.BATCH_SIZE)\n",
    "        \n",
    "        \n",
    "        info_dic = {'Epoch': [self.current_epoch], 'AUC': [auc], 'PRC': [prc], 'Accuracy': [acc], 'loss':[total_loss.item()]}  \n",
    "        \n",
    "        if self.print:\n",
    "            print(f\"\\tVal {info_dic}\")\n",
    "#         print(f\"\\tVal loss: {total_loss.item()}; mean_mae: {epoch_mae.mean()};\" + \n",
    "#              f\" mean_heart_hat: {heart_hat.mean()}; mean_heart_true: {heart_true.mean()}\")\n",
    "\n",
    "        if not os.path.isfile(self.val_fpath):\n",
    "            pd.DataFrame.from_dict(info_dic).to_csv(self.val_fpath)\n",
    "            \n",
    "        else:\n",
    "            pd.concat([pd.read_csv(self.val_fpath, index_col=0), pd.DataFrame.from_dict(info_dic)]).to_csv(self.val_fpath)\n",
    "        \n",
    "        self.val_auc = np.append(self.val_auc, auc)\n",
    "        self.val_heart_true_epoch = np.array([])\n",
    "        self.val_heart_hat_epoch = np.array([])\n",
    "        self.val_loss_epoch = np.append(self.val_loss_epoch, total_loss.item())\n",
    "\n",
    "        \n",
    "        return None\n",
    "        \n",
    "    def get_classbalance_weights(self, y_hat, y_true, beta=0.999):\n",
    "        # Get class counts\n",
    "        classes, counts = torch.unique(y_true, return_counts=True)\n",
    "#         print(classes,counts)\n",
    "        if 1 not in classes: # Case where there are no sick samples\n",
    "            counts = torch.tensor([counts[0], 0]).type_as(y_hat)#.device)\n",
    "        if 0 not in classes: # Case where there are no healthy samples\n",
    "            counts = torch.tensor([0, counts[0]]).type_as(y_hat)#.device)\n",
    "        # Calculate weight for each class\n",
    "        beta = torch.tensor(beta).type_as(y_hat)#.device)        \n",
    "        one = torch.tensor(1.).type_as(y_hat)#.device)\n",
    "        weights = (one - torch.pow(beta, counts)) / (one - beta)\n",
    "        return weights        \n",
    "    \n",
    "    def cbe(self, y_hat, y_true, beta=0.9):\n",
    "        weights = self.get_classbalance_weights(y_hat, y_true, beta=beta)\n",
    "#         print(weights)\n",
    "#         print(torch.unique(y_hat))\n",
    "        cb_ce_loss = F.cross_entropy(y_hat, y_true, weight=weights)\n",
    "        return cb_ce_loss \n",
    "    \n",
    "    def mae(self, y_hat, y_true):\n",
    "        y_true = y_true.view(-1,1)\n",
    "        return torch.abs(y_true - y_hat)\n",
    "    \n",
    "    def softmax_np(x, axis=1):\n",
    "        return np.exp(x)/np.sum(np.exp(x), axis=axis).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07f25e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61280aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet152(weights=ResNet152_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e02a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_auc\", min_delta=0.01, patience=8, verbose=False, mode=\"max\",stopping_threshold = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d13dd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hsiao_base_full_9_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | model     | ResNet     | 60.2 M\n",
      "1 | regresser | Sequential | 8.4 M \n",
      "2 | layers    | Sequential | 58.1 M\n",
      "-----------------------------------------\n",
      "68.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "68.6 M    Total params\n",
      "274.375   Total estimated model params size (MB)\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [0], 'AUC': [0.5431818181818182], 'PRC': [0.0], 'Accuracy': [0.3125], 'loss': [0.7318562269210815]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [0], 'AUC': [0.5919065508576044], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.6277179718017578]}\n",
      "Epoch 0\n",
      "\tTrain {'Epoch': [0], 'AUC': [0.5195407295947679], 'PRC': [0.6482712206612266], 'Accuracy': [0.6179766829635202], 'loss': [0.6518776416778564]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [1], 'AUC': [0.6382146637493552], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.6174633502960205]}\n",
      "Epoch 1\n",
      "\tTrain {'Epoch': [1], 'AUC': [0.5943729007770897], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.6191048622131348]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [2], 'AUC': [0.6811406303979859], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.6129528284072876]}\n",
      "Epoch 2\n",
      "\tTrain {'Epoch': [2], 'AUC': [0.6420886692531006], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.6112375855445862]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [3], 'AUC': [0.686207145414936], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.6065008044242859]}\n",
      "Epoch 3\n",
      "\tTrain {'Epoch': [3], 'AUC': [0.6676974048746221], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.6060354709625244]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [4], 'AUC': [0.7373297687236657], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.57956463098526]}\n",
      "Epoch 4\n",
      "\tTrain {'Epoch': [4], 'AUC': [0.702793617183007], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5908640027046204]};\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [5], 'AUC': [0.7448548074905705], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5717059969902039]}\n",
      "Epoch 5\n",
      "\tTrain {'Epoch': [5], 'AUC': [0.7417149727645479], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5708128809928894]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [6], 'AUC': [0.7485157757245822], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5649751424789429]}\n",
      "Epoch 6\n",
      "\tTrain {'Epoch': [6], 'AUC': [0.7490788963425518], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5651375651359558]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [7], 'AUC': [0.7531451278321926], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5636783242225647]}\n",
      "Epoch 7\n",
      "\tTrain {'Epoch': [7], 'AUC': [0.7555631296306042], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5589508414268494]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [8], 'AUC': [0.7557702743419091], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5584231615066528]}\n",
      "Epoch 8\n",
      "\tTrain {'Epoch': [8], 'AUC': [0.757506349388985], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5559231042861938]};\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [9], 'AUC': [0.7583477016707234], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5579479336738586]}\n",
      "Epoch 9\n",
      "\tTrain {'Epoch': [9], 'AUC': [0.7675141380256625], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5521930456161499]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [10], 'AUC': [0.7571172566755584], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.561895489692688]}\n",
      "Epoch 10\n",
      "\tTrain {'Epoch': [10], 'AUC': [0.7637177103330799], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5527491569519043]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [11], 'AUC': [0.7571492009206252], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5606255531311035]}\n",
      "Epoch 11\n",
      "\tTrain {'Epoch': [11], 'AUC': [0.7658180110067638], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5508469939231873]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [12], 'AUC': [0.7561190976105705], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5583324432373047]}\n",
      "Epoch 12\n",
      "\tTrain {'Epoch': [12], 'AUC': [0.7662394784279342], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5508326292037964]};\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [13], 'AUC': [0.7591829845232104], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5539815425872803]}\n",
      "Epoch 13\n",
      "\tTrain {'Epoch': [13], 'AUC': [0.7681454967676513], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5497015118598938]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [14], 'AUC': [0.7573282464423576], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5558784008026123]}\n",
      "Epoch 14\n",
      "\tTrain {'Epoch': [14], 'AUC': [0.7657589693945923], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5516732335090637]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [15], 'AUC': [0.7580258929796803], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5584349036216736]}\n",
      "Epoch 15\n",
      "\tTrain {'Epoch': [15], 'AUC': [0.7677878032070472], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5498353242874146]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [16], 'AUC': [0.7602852581647124], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5575231909751892]}\n",
      "Epoch 16\n",
      "\tTrain {'Epoch': [16], 'AUC': [0.7666886579948091], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5503860116004944]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [17], 'AUC': [0.7573826699709899], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.560482382774353]}\n",
      "Epoch 17\n",
      "\tTrain {'Epoch': [17], 'AUC': [0.7709518934989411], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5486514568328857]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [18], 'AUC': [0.7582136147161221], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5546874403953552]}\n",
      "Epoch 18\n",
      "\tTrain {'Epoch': [18], 'AUC': [0.7676727674515658], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5498156547546387]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [19], 'AUC': [0.7574769252126067], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5563024878501892]}\n",
      "Epoch 19\n",
      "\tTrain {'Epoch': [19], 'AUC': [0.7690494305395625], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5506306886672974]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [20], 'AUC': [0.7570001277769802], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5610601902008057]}\n",
      "Epoch 20\n",
      "\tTrain {'Epoch': [20], 'AUC': [0.7705591912106835], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5490480661392212]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [21], 'AUC': [0.7580704571734154], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [0.5591524839401245]}\n",
      "Epoch 21\n",
      "\tTrain {'Epoch': [21], 'AUC': [0.7684468864828256], 'PRC': [0.6471605866867244], 'Accuracy': [0.6471605866867244], 'loss': [0.5493676066398621]};\n",
      "mim_base_full_5_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | model     | ResNet     | 60.2 M\n",
      "1 | regresser | Sequential | 8.4 M \n",
      "2 | layers    | Sequential | 58.1 M\n",
      "-----------------------------------------\n",
      "68.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "68.6 M    Total params\n",
      "274.375   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [0], 'AUC': [0.5470588235294118], 'PRC': [0.45614035087719296], 'Accuracy': [0.453125], 'loss': [0.6938146352767944]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [0], 'AUC': [0.7079252862209429], 'PRC': [0.5036697247706422], 'Accuracy': [0.5130588756086764], 'loss': [12.764410972595215]}\n",
      "Epoch 0\n",
      "\tTrain {'Epoch': [0], 'AUC': [0.6081045515979637], 'PRC': [0.5042355513048231], 'Accuracy': [0.519727740578828], 'loss': [0.6869745254516602]};\n",
      "hsiao_base_seg_5_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | model     | ResNet     | 60.2 M\n",
      "1 | regresser | Sequential | 8.4 M \n",
      "2 | layers    | Sequential | 58.1 M\n",
      "-----------------------------------------\n",
      "68.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "68.6 M    Total params\n",
      "274.375   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [0], 'AUC': [0.5], 'PRC': [0.0], 'Accuracy': [0.3125], 'loss': [113700528.0]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [0], 'AUC': [0.7510484416729636], 'PRC': [0.6432009626955475], 'Accuracy': [0.6432009626955475], 'loss': [6765.37353515625]}\n",
      "Epoch 0\n",
      "\tTrain {'Epoch': [0], 'AUC': [0.6690908290413574], 'PRC': [0.6476700007753741], 'Accuracy': [0.6393380970289583], 'loss': [0.5816431045532227]};\n",
      "mim_base_seg_5_reg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | model     | ResNet     | 60.2 M\n",
      "1 | regresser | Sequential | 8.4 M \n",
      "2 | layers    | Sequential | 58.1 M\n",
      "-----------------------------------------\n",
      "68.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "68.6 M    Total params\n",
      "274.375   Total estimated model params size (MB)\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [0], 'AUC': [0.396078431372549], 'PRC': [0.47058823529411764], 'Accuracy': [0.515625], 'loss': [0.7025482058525085]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [0], 'AUC': [0.8570322078915762], 'PRC': [0.7737676056338029], 'Accuracy': [0.7813191677733511], 'loss': [1298.0115966796875]}\n",
      "Epoch 0\n",
      "\tTrain {'Epoch': [0], 'AUC': [0.8351965854423133], 'PRC': [0.7399377362686236], 'Accuracy': [0.7517016213823253], 'loss': [0.5059933066368103]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [1], 'AUC': [0.8729284063506398], 'PRC': [0.7836663770634231], 'Accuracy': [0.7950420540061974], 'loss': [918.5261840820312]}\n",
      "Epoch 1\n",
      "\tTrain {'Epoch': [1], 'AUC': [0.876631452297989], 'PRC': [0.7860096100122919], 'Accuracy': [0.7961374578053234], 'loss': [0.4458441436290741]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [2], 'AUC': [0.8836160264913122], 'PRC': [0.8178331735378715], 'Accuracy': [0.799468791500664], 'loss': [40.939056396484375]}\n",
      "Epoch 2\n",
      "\tTrain {'Epoch': [2], 'AUC': [0.8891362441620645], 'PRC': [0.7952281930611799], 'Accuracy': [0.8114105472857064], 'loss': [0.4258676767349243]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [3], 'AUC': [0.88121321304371], 'PRC': [0.793073593073593], 'Accuracy': [0.8056662239929172], 'loss': [65.26011657714844]}\n",
      "Epoch 3\n",
      "\tTrain {'Epoch': [3], 'AUC': [0.896372804647217], 'PRC': [0.7984001729542752], 'Accuracy': [0.8183830446571856], 'loss': [0.4132883846759796]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [4], 'AUC': [0.8608727896468139], 'PRC': [0.6731984829329962], 'Accuracy': [0.7485613103142984], 'loss': [52.9088020324707]}\n",
      "Epoch 4\n",
      "\tTrain {'Epoch': [4], 'AUC': [0.8990005283246174], 'PRC': [0.8065725628584032], 'Accuracy': [0.8205411986055006], 'loss': [0.40981751680374146]};\n",
      "Epoch 00005: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [5], 'AUC': [0.8908965904351562], 'PRC': [0.7868185516680228], 'Accuracy': [0.8180610889774237], 'loss': [4051.857666015625]}\n",
      "Epoch 5\n",
      "\tTrain {'Epoch': [5], 'AUC': [0.9329466290648847], 'PRC': [0.8434093161546086], 'Accuracy': [0.8580045376570196], 'loss': [0.34021759033203125]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [6], 'AUC': [0.8888277406184443], 'PRC': [0.8156723063223509], 'Accuracy': [0.8198317839752103], 'loss': [5992.43505859375]}\n",
      "Epoch 6\n",
      "\tTrain {'Epoch': [6], 'AUC': [0.9522294051087363], 'PRC': [0.8719982128895343], 'Accuracy': [0.8814675446848542], 'loss': [0.2907984256744385]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [7], 'AUC': [0.8786771277246257], 'PRC': [0.7930449533502969], 'Accuracy': [0.8118636564851704], 'loss': [5473.134765625]}\n",
      "Epoch 7\n",
      "\tTrain {'Epoch': [7], 'AUC': [0.9709298584353795], 'PRC': [0.9076591887604804], 'Accuracy': [0.9110729898732777], 'loss': [0.2327253222465515]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [8], 'AUC': [0.8697126344869973], 'PRC': [0.8084326306141155], 'Accuracy': [0.8038955289951306], 'loss': [18852.34765625]}\n",
      "Epoch 8\n",
      "\tTrain {'Epoch': [8], 'AUC': [0.9835144186877478], 'PRC': [0.9335220838052095], 'Accuracy': [0.9365281390072492], 'loss': [0.17808781564235687]};\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [9], 'AUC': [0.8596968613690315], 'PRC': [0.8069216757741348], 'Accuracy': [0.8043382027445772], 'loss': [21163.650390625]}\n",
      "Epoch 9\n",
      "\tTrain {'Epoch': [9], 'AUC': [0.9939963379102672], 'PRC': [0.9608621667612025], 'Accuracy': [0.962481323667755], 'loss': [0.12385132163763046]};\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_520/904145092.py:113: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n",
      "/tmp/ipykernel_520/904145092.py:135: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  arged = F.softmax(y_hat.detach())[:,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tVal {'Epoch': [10], 'AUC': [0.8574814124936891], 'PRC': [0.8053016453382084], 'Accuracy': [0.8016821602478973], 'loss': [20548.251953125]}\n",
      "Epoch 10\n",
      "\tTrain {'Epoch': [10], 'AUC': [0.9951210868935635], 'PRC': [0.9670342162100716], 'Accuracy': [0.9675723534945493], 'loss': [0.11133746057748795]};\n"
     ]
    }
   ],
   "source": [
    "final_lin = [[2048, 4096], [4096, 2]]\n",
    "titles = ['hsiao_base_full_9_reg', 'mim_base_full_5_reg', 'hsiao_base_seg_5_reg', 'mim_base_seg_5_reg']\n",
    "train_vals = [[hsiao_full_train_dl, hsiao_full_val_dl], [mim_full_train_dl, mim_full_val_dl],\n",
    "              [hsiao_seg_train_dl, hsiao_seg_val_dl], [mim_seg_train_dl, mim_seg_val_dl]]\n",
    "\n",
    "for i in range(4):\n",
    "    print(titles[i])\n",
    "    net = ClassyNet(title= titles[i], layer_defs=None, linear_layers = final_lin, is_transfer=True, \n",
    "       model = model, lr_scheduler=True, lr = 1e-5, print_on = True)\n",
    "    #net.load_state_dict(torch.load(f'models/{titles[i]}'))\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator='gpu',\n",
    "        max_epochs=75, \n",
    "        enable_progress_bar=False,\n",
    "        check_val_every_n_epoch = 1,\n",
    "        callbacks=[early_stop_callback],\n",
    "        logger=False,\n",
    "        enable_checkpointing=False)\n",
    "    net.train()\n",
    "    trainer.fit(net, train_vals[i][0], train_vals[i][1])\n",
    "    torch.save(net.state_dict(), f'models/{titles[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71579e77",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98ca4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
